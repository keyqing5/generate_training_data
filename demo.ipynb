{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen-plus LLM 客户端初始化成功。\n",
      "正在处理文件: .\\my_python_repo\\my_module.py\n",
      "\n",
      "训练数据已生成并保存到 `scenario1_qa_data_with_llm.jsonl`。共生成 5 条记录。\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import os\n",
    "import json\n",
    "import inspect\n",
    "from typing import List, Dict, Any, Optional\n",
    "from openai import OpenAI\n",
    "import getpass # 用于安全输入API Key，防止泄露\n",
    "\n",
    "# --- Configuration ---\n",
    "CODE_REPO_PATH = \".\\my_python_repo\" # 替换为你的本地Python代码仓库路径\n",
    "OUTPUT_FILE = \"scenario1_qa_data_with_llm.jsonl\"\n",
    "LLM_MODEL_NAME = \"qwen-plus\" # 使用您指定的模型\n",
    "\n",
    "# --- OpenAI Client Initialization ---\n",
    "# 建议通过环境变量设置 DASHSCOPE_API_KEY，或在运行时安全输入\n",
    "DASHSCOPE_API_KEY = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "if not DASHSCOPE_API_KEY:\n",
    "    DASHSCOPE_API_KEY = getpass.getpass(\"请输入您的 DASHSCOPE_API_KEY：\")\n",
    "\n",
    "try:\n",
    "    llm_client = OpenAI(\n",
    "        api_key=DASHSCOPE_API_KEY,\n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\", # 北京地域base_url\n",
    "    )\n",
    "    print(\"Qwen-plus LLM 客户端初始化成功。\")\n",
    "except Exception as e:\n",
    "    print(f\"错误信息：LLM 客户端初始化失败 - {e}\")\n",
    "    print(\"请参考文档：https://help.aliyun.com/zh/model-studio/developer-reference/error-code\")\n",
    "    llm_client = None # 将客户端设置为None，后续调用会跳过\n",
    "\n",
    "# --- Helper Functions (unchanged from previous example) ---\n",
    "def extract_function_info(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    从单个Python文件中提取函数及其文档字符串和代码片段。\n",
    "    \"\"\"\n",
    "    functions_info = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        tree = ast.parse(f.read(), filename=file_path)\n",
    "\n",
    "    source_lines = open(file_path, \"r\", encoding=\"utf-8\").readlines()\n",
    "\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
    "            function_name = node.name\n",
    "            docstring = ast.get_docstring(node)\n",
    "\n",
    "            try:\n",
    "                start_line = node.lineno\n",
    "                end_line = node.end_lineno if node.end_lineno is not None else start_line\n",
    "                snippet_lines = source_lines[start_line - 1:end_line]\n",
    "                snippet = \"\".join(snippet_lines)\n",
    "            except Exception as e:\n",
    "                snippet = f\"# Error extracting snippet for {function_name}: {e}\\n\"\n",
    "                print(f\"Warning: Could not extract snippet for {function_name} in {file_path}. Error: {e}\")\n",
    "\n",
    "            functions_info.append({\n",
    "                \"function_name\": function_name,\n",
    "                \"docstring\": docstring if docstring else \"\",\n",
    "                \"code_snippet\": snippet,\n",
    "                \"start_line\": start_line,\n",
    "                \"end_line\": end_line\n",
    "            })\n",
    "    return functions_info\n",
    "\n",
    "# --- LLM Integration Function ---\n",
    "def call_llm_for_qa(function_name: str, docstring: str, code_snippet: str, file_path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    调用LLM生成问答对的答案和推理trace。\n",
    "    \"\"\"\n",
    "    if not llm_client:\n",
    "        return {\n",
    "            \"answer\": \"LLM客户端未初始化，无法生成答案。\",\n",
    "            \"inference_trace\": \"LLM客户端初始化失败。\"\n",
    "        }\n",
    "\n",
    "    # 构建Prompt\n",
    "    system_prompt = (\n",
    "        \"你是一个专业的代码分析助手，能够理解Python代码并解释其功能。\\n\"\n",
    "        \"请根据提供的函数信息，首先一步步思考，然后给出函数的主要功能解释和相关的推理过程。\\n\"\n",
    "        \"你的回答应包含两部分：'answer' 和 'inference_trace'。\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"请分析以下Python函数，并回答它的主要功能是什么？\\n\\n\"\n",
    "        f\"--- 函数信息 ---\\n\"\n",
    "        f\"文件路径: {file_path}\\n\"\n",
    "        f\"函数名: {function_name}\\n\"\n",
    "        f\"文档字符串:\\n```\\n{docstring if docstring else '无文档字符串'}\\n```\\n\"\n",
    "        f\"代码片段:\\n```python\\n{code_snippet}\\n```\\n\\n\"\n",
    "        f\"--- 输出格式 ---\\n\"\n",
    "        f\"请以JSON格式返回你的回答，其中包含 'answer' (对函数功能的解释) 和 'inference_trace' (你思考并得出答案的步骤)。\\n\"\n",
    "        f\"例如：\\n\"\n",
    "        f\"```json\\n\"\n",
    "        f\"{{\\n\"\n",
    "        f\"  \\\"answer\\\": \\\"函数的主要功能是...\\\",\\n\"\n",
    "        f\"  \\\"inference_trace\\\": \\\"1. 首先我识别到...\\\\n2. 接着我分析了...\\\\n3. 最终我得出结论...\\\"\\n\"\n",
    "        f\"}}\\n\"\n",
    "        f\"```\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        completion = llm_client.chat.completions.create(\n",
    "            model=LLM_MODEL_NAME,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': system_prompt},\n",
    "                {'role': 'user', 'content': user_prompt}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"} # 明确要求LLM以JSON格式返回\n",
    "        )\n",
    "        llm_response_content = completion.choices[0].message.content\n",
    "\n",
    "        # 尝试解析LLM的JSON响应\n",
    "        try:\n",
    "            parsed_response = json.loads(llm_response_content)\n",
    "            return {\n",
    "                \"answer\": parsed_response.get(\"answer\", \"LLM未提供答案。\"),\n",
    "                \"inference_trace\": parsed_response.get(\"inference_trace\", \"LLM未提供推理trace。\")\n",
    "            }\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: LLM 返回的不是有效的 JSON 格式。\\n原始响应: {llm_response_content}\")\n",
    "            return {\n",
    "                \"answer\": f\"LLM返回无效格式，原始响应：{llm_response_content}\",\n",
    "                \"inference_trace\": \"LLM返回无效JSON。\"\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"LLM 调用失败: {e}\")\n",
    "        return {\n",
    "            \"answer\": f\"LLM调用失败，错误：{e}\",\n",
    "            \"inference_trace\": f\"LLM调用失败，错误：{e}\"\n",
    "        }\n",
    "\n",
    "def generate_qa_for_function(\n",
    "    file_path: str,\n",
    "    func_info: Dict[str, Any],\n",
    "    qa_id_counter: List[int]\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    为单个函数生成一个简化的问答对，并集成LLM调用。\n",
    "    \"\"\"\n",
    "    function_name = func_info[\"function_name\"]\n",
    "    docstring = func_info[\"docstring\"]\n",
    "    code_snippet = func_info[\"code_snippet\"]\n",
    "    start_line = func_info[\"start_line\"]\n",
    "    end_line = func_info[\"end_line\"]\n",
    "\n",
    "    if not code_snippet.strip(): # 如果代码片段为空，则跳过\n",
    "        return None\n",
    "\n",
    "    qa_id_counter[0] += 1\n",
    "    qa_id = f\"qa_py_{qa_id_counter[0]:05d}\"\n",
    "\n",
    "    question = f\"函数 `{function_name}` (在 `{os.path.basename(file_path)}` 中) 的主要功能是什么？\"\n",
    "\n",
    "    # 调用LLM生成答案和推理trace\n",
    "    llm_output = call_llm_for_qa(function_name, docstring, code_snippet, file_path)\n",
    "    answer = llm_output[\"answer\"]\n",
    "    inference_trace = llm_output[\"inference_trace\"]\n",
    "\n",
    "    # 模拟业务规则（这里是通用的，实际应从项目文档或特定注释中提取）\n",
    "    business_rules = []\n",
    "    if \"save\" in function_name.lower() or \"update\" in function_name.lower():\n",
    "        business_rules.append(\"数据持久化操作需考虑事务一致性。\")\n",
    "    if \"auth\" in function_name.lower() or \"login\" in function_name.lower():\n",
    "        business_rules.append(\"用户认证和授权操作需遵循安全最佳实践。\")\n",
    "\n",
    "    qa_data = {\n",
    "        \"id\": qa_id,\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"code_context\": [\n",
    "            {\n",
    "                \"file_path\": os.path.relpath(file_path, CODE_REPO_PATH),\n",
    "                \"line_start\": start_line,\n",
    "                \"line_end\": end_line,\n",
    "                \"snippet\": code_snippet.strip()\n",
    "            }\n",
    "        ],\n",
    "        \"business_rules_context\": business_rules,\n",
    "        \"inference_trace\": inference_trace,\n",
    "        \"metadata\": {\n",
    "            \"source_module\": os.path.dirname(os.path.relpath(file_path, CODE_REPO_PATH)),\n",
    "            \"language\": \"python\",\n",
    "            \"difficulty\": \"llm_generated\", # 标记为LLM生成\n",
    "            \"timestamp\": \"2025-12-09T\" + os.popen('date -u +\"%H:%M:%SZ\"').read().strip(),\n",
    "            \"version_control_hash\": \"dummy_hash_for_example\"\n",
    "        }\n",
    "    }\n",
    "    return qa_data\n",
    "\n",
    "# --- Main Script ---\n",
    "def main():\n",
    "    if not os.path.exists(CODE_REPO_PATH):\n",
    "        print(f\"错误: 代码仓库路径 `{CODE_REPO_PATH}` 不存在。请修改 CODE_REPO_PATH 为你的本地Python仓库路径。\")\n",
    "        print(\"尝试创建一个简单的虚拟仓库用于演示...\")\n",
    "        os.makedirs(CODE_REPO_PATH, exist_ok=True)\n",
    "        with open(os.path.join(CODE_REPO_PATH, \"my_module.py\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\"\"\n",
    "def calculate_sum(a: int, b: int) -> int:\n",
    "    \\\"\\\"\\\"\n",
    "    计算两个整数的和。\n",
    "    这个函数接受两个整数作为输入，并返回它们的和。\n",
    "    \\\"\\\"\\\"\n",
    "    return a + b\n",
    "\n",
    "def process_data(data_list: list):\n",
    "    # This function processes a list of data without a docstring.\n",
    "    print(f\"Processing {len(data_list)} items.\")\n",
    "    for item in data_list:\n",
    "        if item % 2 == 0:\n",
    "            print(f\"Even item: {item}\")\n",
    "        else:\n",
    "            print(f\"Odd item: {item}\")\n",
    "\n",
    "class MyManager:\n",
    "    \\\"\\\"\\\"\n",
    "    一个用于管理资源的类。\n",
    "    提供了资源的创建、读取、更新和删除(CRUD)操作。\n",
    "    \\\"\\\"\\\"\n",
    "    def __init__(self, resource_name: str):\n",
    "        self.resource_name = resource_name\n",
    "        self.resources = []\n",
    "\n",
    "    def create_resource(self, resource_data: dict):\n",
    "        \\\"\\\"\\\"\n",
    "        创建一个新资源并添加到管理器中。\n",
    "        :param resource_data: 资源的字典数据。\n",
    "        :return: None\n",
    "        \\\"\\\"\\\"\n",
    "        self.resources.append(resource_data)\n",
    "        print(f\"Resource created: {resource_data}\")\n",
    "\n",
    "    def get_resource(self, resource_id: str) -> Optional[dict]:\n",
    "        # 从管理器中获取指定ID的资源。\n",
    "        # 这是一个查找资源的示例方法。\n",
    "        for res in self.resources:\n",
    "            if res.get(\"id\") == resource_id:\n",
    "                return res\n",
    "        return None\n",
    "\"\"\")\n",
    "        print(f\"已创建虚拟仓库于 `{CODE_REPO_PATH}`。\")\n",
    "\n",
    "    all_qa_data = []\n",
    "    qa_id_counter = [0]\n",
    "\n",
    "    for root, _, files in os.walk(CODE_REPO_PATH):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                print(f\"正在处理文件: {file_path}\")\n",
    "                functions_info = extract_function_info(file_path)\n",
    "                for func_info in functions_info:\n",
    "                    qa_entry = generate_qa_for_function(file_path, func_info, qa_id_counter)\n",
    "                    if qa_entry:\n",
    "                        all_qa_data.append(qa_entry)\n",
    "\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in all_qa_data:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"\\n训练数据已生成并保存到 `{OUTPUT_FILE}`。共生成 {len(all_qa_data)} 条记录。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen-plus LLM 客户端初始化成功。\n",
      "\n",
      "正在为需求生成设计方案: 为现有订单系统增加一个异步的库存扣减服务，以提高订单处理的响应速度，并确保库存数据最终一致性。\n",
      "\n",
      "正在为需求生成设计方案: 实现一个用户权限管理模块，支持角色-权限分配，并提供API进行权限校验。\n",
      "\n",
      "正在为需求生成设计方案: 优化支付流程，引入重试机制和幂等性处理，提高支付成功率。\n",
      "\n",
      "正在为需求生成设计方案: 为系统添加一个统一的错误日志和监控报警机制。\n",
      "\n",
      "正在为需求生成设计方案: 将用户注册和登录功能从现有用户管理模块中独立出来，形成一个独立的认证服务。\n",
      "\n",
      "训练数据已生成并保存到 `scenario2_design_data_with_llm.jsonl`。共生成 5 条记录。\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "from openai import OpenAI\n",
    "import getpass\n",
    "import datetime\n",
    "\n",
    "# --- Configuration ---\n",
    "CODE_REPO_PATH = \"./my_python_repo\" # 替换为你的本地Python代码仓库路径\n",
    "OUTPUT_FILE = \"scenario2_design_data_with_llm.jsonl\"\n",
    "LLM_MODEL_NAME = \"qwen-plus\" # 使用您指定的模型\n",
    "REQUIREMENTS = [\n",
    "    {\"req\": \"为现有订单系统增加一个异步的库存扣减服务，以提高订单处理的响应速度，并确保库存数据最终一致性。\", \"keywords\": [\"order\", \"inventory\", \"async\"]},\n",
    "    {\"req\": \"实现一个用户权限管理模块，支持角色-权限分配，并提供API进行权限校验。\", \"keywords\": [\"user\", \"auth\", \"permission\", \"role\"]},\n",
    "    {\"req\": \"优化支付流程，引入重试机制和幂等性处理，提高支付成功率。\", \"keywords\": [\"payment\", \"retry\", \"idempotent\"]},\n",
    "    {\"req\": \"为系统添加一个统一的错误日志和监控报警机制。\", \"keywords\": [\"log\", \"monitor\", \"error\"]},\n",
    "    {\"req\": \"将用户注册和登录功能从现有用户管理模块中独立出来，形成一个独立的认证服务。\", \"keywords\": [\"user\", \"register\", \"login\", \"auth\", \"service\"]},\n",
    "]\n",
    "\n",
    "# --- OpenAI Client Initialization ---\n",
    "DASHSCOPE_API_KEY = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "if not DASHSCOPE_API_KEY:\n",
    "    DASHSCOPE_API_KEY = getpass.getpass(\"请输入您的 DASHSCOPE_API_KEY：\")\n",
    "\n",
    "try:\n",
    "    llm_client = OpenAI(\n",
    "        api_key=DASHSCOPE_API_KEY,\n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\", # 北京地域base_url\n",
    "    )\n",
    "    print(\"Qwen-plus LLM 客户端初始化成功。\")\n",
    "except Exception as e:\n",
    "    print(f\"错误信息：LLM 客户端初始化失败 - {e}\")\n",
    "    print(\"请参考文档：https://help.aliyun.com/zh/model-studio/developer-reference/error-code\")\n",
    "    llm_client = None\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def get_repo_file_list(repo_path: str, keywords: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    获取代码仓库中所有Python文件的相对路径，\n",
    "    并模拟根据关键词筛选相关文件。\n",
    "    在真实场景中，这里会是复杂的知识图谱查询或向量搜索。\n",
    "    \"\"\"\n",
    "    relevant_files = []\n",
    "    all_files = []\n",
    "    for root, _, files in os.walk(repo_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(\".py\"):\n",
    "                relative_path = os.path.relpath(os.path.join(root, file_name), repo_path)\n",
    "                all_files.append(relative_path)\n",
    "\n",
    "                # 模拟关键词匹配：如果文件路径或内容包含关键词，则认为相关\n",
    "                # 注意：这里为了简化，只检查文件路径和文件名\n",
    "                is_relevant = False\n",
    "                for keyword in keywords:\n",
    "                    if keyword.lower() in relative_path.lower():\n",
    "                        is_relevant = True\n",
    "                        break\n",
    "                if is_relevant:\n",
    "                    relevant_files.append(relative_path)\n",
    "\n",
    "    # 如果没有找到相关文件，则返回所有文件中的一小部分作为通用上下文\n",
    "    if not relevant_files and all_files:\n",
    "        return all_files[:min(5, len(all_files))] # 返回最多前5个文件\n",
    "    elif relevant_files:\n",
    "        return relevant_files\n",
    "    return []\n",
    "\n",
    "def read_file_content(repo_path: str, relative_file_path: str) -> str:\n",
    "    \"\"\"读取指定文件的内容。\"\"\"\n",
    "    full_path = os.path.join(repo_path, relative_file_path)\n",
    "    try:\n",
    "        with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        return f\"# Error reading {relative_file_path}: {e}\"\n",
    "\n",
    "# --- LLM Integration Function for Scenario 2 ---\n",
    "def call_llm_for_design_solution(\n",
    "    requirement: str,\n",
    "    codebase_context: List[str], # 相关文件列表\n",
    "    code_contents: Dict[str, str] # 相关文件内容\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    调用LLM生成架构设计方案、解释和推理trace。\n",
    "    \"\"\"\n",
    "    if not llm_client:\n",
    "        return {\n",
    "            \"design_solution\": \"LLM客户端未初始化，无法生成设计方案。\",\n",
    "            \"explanation\": \"LLM客户端初始化失败。\",\n",
    "            \"inference_trace\": \"LLM客户端初始化失败。\"\n",
    "        }\n",
    "\n",
    "    # 构建Prompt\n",
    "    system_prompt = (\n",
    "        \"你是一个资深的软件架构师和代码专家，能够根据给定的需求和现有代码仓信息，提供详细、合理且可扩展的架构设计方案。\\n\"\n",
    "        \"请先一步步思考，分析需求和现有上下文，然后给出设计方案、解释和推理过程。\\n\"\n",
    "        \"你的回答应包含三部分：'design_solution' (设计方案), 'explanation' (设计方案的解释), 和 'inference_trace' (你思考并得出方案的步骤)。\"\n",
    "    )\n",
    "\n",
    "    context_str = \"\"\n",
    "    if codebase_context:\n",
    "        context_str += \"\\n--- 现有代码仓相关文件 ---\\n\"\n",
    "        for i, file_path in enumerate(codebase_context):\n",
    "            context_str += f\"文件 {i+1}: {file_path}\\n\"\n",
    "            # 实际中这里不会直接放所有文件内容，而是通过知识图谱提取关键信息或使用RAG获取摘要\n",
    "            # 为了演示，这里假设可以提供部分文件内容\n",
    "            if file_path in code_contents:\n",
    "                # 仅展示部分内容，避免Prompt过长\n",
    "                content = code_contents[file_path]\n",
    "                context_str += f\"```python\\n{content[:500]}...\\n```\\n\" # 截断\n",
    "            context_str += \"---\\n\"\n",
    "    else:\n",
    "        context_str += \"\\n--- 现有代码仓信息 ---\\n\"\n",
    "        context_str += \"未找到与需求直接相关的代码文件，请基于通用设计原则和最佳实践进行设计。\\n\"\n",
    "\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"请根据以下需求和提供的现有代码仓上下文，设计一个架构方案。请注意：\\n\"\n",
    "        f\"1. 你的设计应考虑现有Python代码仓的特点和可能的扩展方向。\\n\"\n",
    "        f\"2. 方案应结构化、清晰，并包含必要的解释和推理过程。\\n\\n\"\n",
    "        f\"--- 需求 ---\\n\"\n",
    "        f\"{requirement}\\n\"\n",
    "        f\"{context_str}\\n\\n\"\n",
    "        f\"--- 输出格式 ---\\n\"\n",
    "        f\"请以JSON格式返回你的回答，其中包含 'design_solution' (详细的架构设计方案，使用Markdown格式),\\n\"\n",
    "        f\" 'explanation' (对设计方案的解释) 和 'inference_trace' (你思考并得出方案的步骤)。\\n\"\n",
    "        f\"例如：\\n\"\n",
    "        f\"```json\\n\"\n",
    "        f\"{{\\n\"\n",
    "        f\"  \\\"design_solution\\\": \\\"# 方案标题\\\\n1. ...\\\\n2. ...\\\",\\n\"\n",
    "        f\"  \\\"explanation\\\": \\\"此方案的优点是...\\\",\\n\"\n",
    "        f\"  \\\"inference_trace\\\": \\\"1. 首先我分析了需求...\\\\n2. 接着我评估了现有系统...\\\\n3. 最终我提出了...\\\"\\n\"\n",
    "        f\"}}\\n\"\n",
    "        f\"```\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        completion = llm_client.chat.completions.create(\n",
    "            model=LLM_MODEL_NAME,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': system_prompt},\n",
    "                {'role': 'user', 'content': user_prompt}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        llm_response_content = completion.choices[0].message.content\n",
    "\n",
    "        try:\n",
    "            parsed_response = json.loads(llm_response_content)\n",
    "            return {\n",
    "                \"design_solution\": parsed_response.get(\"design_solution\", \"LLM未提供设计方案。\"),\n",
    "                \"explanation\": parsed_response.get(\"explanation\", \"LLM未提供解释。\"),\n",
    "                \"inference_trace\": parsed_response.get(\"inference_trace\", \"LLM未提供推理trace。\")\n",
    "            }\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: LLM 返回的不是有效的 JSON 格式。\\n原始响应: {llm_response_content}\")\n",
    "            return {\n",
    "                \"design_solution\": f\"LLM返回无效格式，原始响应：{llm_response_content}\",\n",
    "                \"explanation\": \"LLM返回无效JSON。\",\n",
    "                \"inference_trace\": \"LLM返回无效JSON。\"\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"LLM 调用失败: {e}\")\n",
    "        return {\n",
    "            \"design_solution\": f\"LLM调用失败，错误：{e}\",\n",
    "            \"explanation\": f\"LLM调用失败，错误：{e}\",\n",
    "            \"inference_trace\": f\"LLM调用失败，错误：{e}\"\n",
    "        }\n",
    "\n",
    "def generate_design_data(\n",
    "    design_id_counter: List[int],\n",
    "    requirement: str,\n",
    "    repo_path: str,\n",
    "    # 模拟从需求中提取关键词，真实情况会更复杂\n",
    "    requirement_keywords: List[str]\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    生成一个架构设计方案的数据点。\n",
    "    \"\"\"\n",
    "    if not requirement.strip():\n",
    "        return None\n",
    "\n",
    "    design_id_counter[0] += 1\n",
    "    design_id = f\"design_py_{design_id_counter[0]:05d}\"\n",
    "\n",
    "    # 模拟上下文提取：获取与需求关键词相关的代码文件列表\n",
    "    codebase_context_files = get_repo_file_list(repo_path, requirement_keywords)\n",
    "\n",
    "    # 为了LLM调用，需要读取这些文件的内容 (这里仅截取部分以避免过长Prompt)\n",
    "    code_contents_for_llm = {\n",
    "        f: read_file_content(repo_path, f) for f in codebase_context_files\n",
    "    }\n",
    "\n",
    "    # 调用LLM生成设计方案\n",
    "    llm_output = call_llm_for_design_solution(\n",
    "        requirement,\n",
    "        codebase_context_files,\n",
    "        code_contents_for_llm\n",
    "    )\n",
    "\n",
    "    design_data = {\n",
    "        \"id\": design_id,\n",
    "        \"requirement\": requirement,\n",
    "        \"design_solution\": llm_output[\"design_solution\"],\n",
    "        \"explanation\": llm_output[\"explanation\"],\n",
    "        \"inference_trace\": llm_output[\"inference_trace\"],\n",
    "        \"codebase_context\": codebase_context_files, # 记录 LLM 看到的相关文件列表\n",
    "        \"metadata\": {\n",
    "            \"source_project\": os.path.basename(repo_path),\n",
    "            \"design_type\": \"feature_extension\", # 默认类型，实际可由LLM判断或根据需求定义\n",
    "            \"language\": \"python\",\n",
    "            \"difficulty\": \"llm_generated\",\n",
    "            \"timestamp\": datetime.datetime.now(datetime.timezone.utc).isoformat(),\n",
    "            \"version_control_hash\": \"dummy_hash_for_example\" # 实际应获取git commit hash\n",
    "        }\n",
    "    }\n",
    "    return design_data\n",
    "\n",
    "# --- Main Script ---\n",
    "def main():\n",
    "    if not os.path.exists(CODE_REPO_PATH):\n",
    "        print(f\"错误: 代码仓库路径 `{CODE_REPO_PATH}` 不存在。请修改 CODE_REPO_PATH 为你的本地Python仓库路径。\")\n",
    "        print(\"尝试创建一个简单的虚拟仓库用于演示...\")\n",
    "        os.makedirs(CODE_REPO_PATH, exist_ok=True)\n",
    "        with open(os.path.join(CODE_REPO_PATH, \"__init__.py\"), \"w\") as f: f.write(\"\")\n",
    "        with open(os.path.join(CODE_REPO_PATH, \"user_management.py\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\"\"\n",
    "def register_user(username, password, email):\n",
    "    # This registers a new user\n",
    "    print(f\"Registering {username}\")\n",
    "    # ... database logic\n",
    "    return {\"id\": 1, \"username\": username}\n",
    "\n",
    "def get_user_profile(user_id):\n",
    "    # Retrieves user profile from DB\n",
    "    return {\"id\": user_id, \"username\": \"test_user\"}\n",
    "\"\"\")\n",
    "        with open(os.path.join(CODE_REPO_PATH, \"order_service.py\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\"\"\n",
    "def create_order(user_id, items):\n",
    "    # Creates a new order\n",
    "    print(f\"Creating order for user {user_id}\")\n",
    "    # ... inventory check, payment processing\n",
    "    return {\"order_id\": \"ORD001\", \"status\": \"pending\"}\n",
    "\n",
    "def update_order_status(order_id, new_status):\n",
    "    # Updates an existing order's status\n",
    "    print(f\"Updating order {order_id} to {new_status}\")\n",
    "    return True\n",
    "\"\"\")\n",
    "        with open(os.path.join(CODE_REPO_PATH, \"payment_gateway.py\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\"\"\n",
    "def process_payment(order_id, amount, payment_method):\n",
    "    # Integrates with external payment provider\n",
    "    print(f\"Processing payment for order {order_id}, amount {amount}\")\n",
    "    return {\"success\": True, \"transaction_id\": \"TXN123\"}\n",
    "\"\"\")\n",
    "        print(f\"已创建虚拟仓库于 `{CODE_REPO_PATH}`。\")\n",
    "\n",
    "\n",
    "\n",
    "    all_design_data = []\n",
    "    design_id_counter = [0]\n",
    "\n",
    "    for req_item in REQUIREMENTS:\n",
    "        print(f\"\\n正在为需求生成设计方案: {req_item['req']}\")\n",
    "        design_entry = generate_design_data(\n",
    "            design_id_counter,\n",
    "            req_item[\"req\"],\n",
    "            CODE_REPO_PATH,\n",
    "            req_item[\"keywords\"]\n",
    "        )\n",
    "        if design_entry:\n",
    "            all_design_data.append(design_entry)\n",
    "\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in all_design_data:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"\\n训练数据已生成并保存到 `{OUTPUT_FILE}`。共生成 {len(all_design_data)} 条记录。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
