{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen-plus LLM client initialized successfully.\n",
      "Processing file: .\\my_python_repo\\my_module.py\n",
      "\n",
      "Training data generated and saved to `scenario1_qa_data_with_llm_en.jsonl`. Total 5 records generated.\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import os\n",
    "import json\n",
    "import inspect\n",
    "from typing import List, Dict, Any, Optional\n",
    "from openai import OpenAI\n",
    "import getpass # For securely inputting API Key, preventing exposure\n",
    "\n",
    "# --- Configuration ---\n",
    "CODE_REPO_PATH = \".\\my_python_repo\" # Replace with your local Python code repository path\n",
    "OUTPUT_FILE = \"scenario1_qa_data_with_llm_en.jsonl\"\n",
    "LLM_MODEL_NAME = \"qwen-plus\" # Use your specified model\n",
    "\n",
    "# --- OpenAI Client Initialization ---\n",
    "# It is recommended to set DASHSCOPE_API_KEY via environment variables, or input it securely at runtime\n",
    "DASHSCOPE_API_KEY = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "if not DASHSCOPE_API_KEY:\n",
    "    DASHSCOPE_API_KEY = getpass.getpass(\"Please enter your DASHSCOPE_API_KEY: \")\n",
    "\n",
    "try:\n",
    "    llm_client = OpenAI(\n",
    "        api_key=DASHSCOPE_API_KEY,\n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\", # Beijing region base_url\n",
    "    )\n",
    "    print(\"Qwen-plus LLM client initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: LLM client initialization failed - {e}\")\n",
    "    print(\"Please refer to the documentation: https://help.aliyun.com/zh/model-studio/developer-reference/error-code\")\n",
    "    llm_client = None # Set client to None, subsequent calls will be skipped\n",
    "\n",
    "# --- Helper Functions (unchanged from previous example) ---\n",
    "def extract_function_info(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extracts functions, their docstrings, and code snippets from a single Python file.\n",
    "    \"\"\"\n",
    "    functions_info = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        tree = ast.parse(f.read(), filename=file_path)\n",
    "\n",
    "    source_lines = open(file_path, \"r\", encoding=\"utf-8\").readlines()\n",
    "\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
    "            function_name = node.name\n",
    "            docstring = ast.get_docstring(node)\n",
    "\n",
    "            try:\n",
    "                start_line = node.lineno\n",
    "                end_line = node.end_lineno if node.end_lineno is not None else start_line\n",
    "                snippet_lines = source_lines[start_line - 1:end_line]\n",
    "                snippet = \"\".join(snippet_lines)\n",
    "            except Exception as e:\n",
    "                snippet = f\"# Error extracting snippet for {function_name}: {e}\\n\"\n",
    "                print(f\"Warning: Could not extract snippet for {function_name} in {file_path}. Error: {e}\")\n",
    "\n",
    "            functions_info.append({\n",
    "                \"function_name\": function_name,\n",
    "                \"docstring\": docstring if docstring else \"\",\n",
    "                \"code_snippet\": snippet,\n",
    "                \"start_line\": start_line,\n",
    "                \"end_line\": end_line\n",
    "            })\n",
    "    return functions_info\n",
    "\n",
    "# --- LLM Integration Function ---\n",
    "def call_llm_for_qa(function_name: str, docstring: str, code_snippet: str, file_path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Calls the LLM to generate answers and inference traces for QA pairs.\n",
    "    \"\"\"\n",
    "    if not llm_client:\n",
    "        return {\n",
    "            \"answer\": \"LLM client not initialized, cannot generate answer.\",\n",
    "            \"inference_trace\": \"LLM client initialization failed.\"\n",
    "        }\n",
    "\n",
    "    # Construct Prompt\n",
    "    system_prompt = (\n",
    "        \"You are a professional code analysis assistant capable of understanding Python code and explaining its functionality.\\n\"\n",
    "        \"Based on the provided function information, please first think step-by-step, then provide a main functionality explanation for the function and its relevant inference process.\\n\"\n",
    "        \"Your answer should consist of two parts: 'answer' and 'inference_trace'.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"Please analyze the following Python function and answer what its main functionality is?\\n\\n\"\n",
    "        f\"--- Function Information ---\\n\"\n",
    "        f\"File Path: {file_path}\\n\"\n",
    "        f\"Function Name: {function_name}\\n\"\n",
    "        f\"Docstring:\\n```\\n{docstring if docstring else 'No docstring'}\\n```\\n\"\n",
    "        f\"Code Snippet:\\n```python\\n{code_snippet}\\n```\\n\\n\"\n",
    "        f\"--- Output Format ---\\n\"\n",
    "        f\"Please return your answer in JSON format, including 'answer' (explanation of function functionality) and 'inference_trace' (steps you thought through to arrive at the answer).\\n\"\n",
    "        f\"Example:\\n\"\n",
    "        f\"```json\\n\"\n",
    "        f\"{{\\n\"\n",
    "        f\"  \\\"answer\\\": \\\"The main functionality of the function is...\\\",\\n\"\n",
    "        f\"  \\\"inference_trace\\\": \\\"1. First I identified...\\\\n2. Then I analyzed...\\\\n3. Finally I concluded...\\\"\\n\"\n",
    "        f\"}}\\n\"\n",
    "        f\"```\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        completion = llm_client.chat.completions.create(\n",
    "            model=LLM_MODEL_NAME,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': system_prompt},\n",
    "                {'role': 'user', 'content': user_prompt}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"} # Explicitly request LLM to return in JSON format\n",
    "        )\n",
    "        llm_response_content = completion.choices[0].message.content\n",
    "\n",
    "        # Attempt to parse LLM's JSON response\n",
    "        try:\n",
    "            parsed_response = json.loads(llm_response_content)\n",
    "            return {\n",
    "                \"answer\": parsed_response.get(\"answer\", \"LLM did not provide an answer.\"),\n",
    "                \"inference_trace\": parsed_response.get(\"inference_trace\", \"LLM did not provide inference trace.\")\n",
    "            }\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: LLM response is not valid JSON format.\\nRaw response: {llm_response_content}\")\n",
    "            return {\n",
    "                \"answer\": f\"LLM returned invalid format, raw response: {llm_response_content}\",\n",
    "                \"inference_trace\": \"LLM returned invalid JSON.\"\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"LLM call failed: {e}\")\n",
    "        return {\n",
    "            \"answer\": f\"LLM call failed, error: {e}\",\n",
    "            \"inference_trace\": f\"LLM call failed, error: {e}\"\n",
    "        }\n",
    "\n",
    "def generate_qa_for_function(\n",
    "    file_path: str,\n",
    "    func_info: Dict[str, Any],\n",
    "    qa_id_counter: List[int]\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generates a simplified QA pair for a single function, integrating LLM calls.\n",
    "    \"\"\"\n",
    "    function_name = func_info[\"function_name\"]\n",
    "    docstring = func_info[\"docstring\"]\n",
    "    code_snippet = func_info[\"code_snippet\"]\n",
    "    start_line = func_info[\"start_line\"]\n",
    "    end_line = func_info[\"end_line\"]\n",
    "\n",
    "    if not code_snippet.strip(): # Skip if code snippet is empty\n",
    "        return None\n",
    "\n",
    "    qa_id_counter[0] += 1\n",
    "    qa_id = f\"qa_py_{qa_id_counter[0]:05d}\"\n",
    "\n",
    "    question = f\"What is the main functionality of function `{function_name}` (in `{os.path.basename(file_path)}`)?\"\n",
    "\n",
    "    # Call LLM to generate answer and inference trace\n",
    "    llm_output = call_llm_for_qa(function_name, docstring, code_snippet, file_path)\n",
    "    answer = llm_output[\"answer\"]\n",
    "    inference_trace = llm_output[\"inference_trace\"]\n",
    "\n",
    "    # Simulate business rules (generic here, should be extracted from project documentation or specific comments)\n",
    "    business_rules = []\n",
    "    if \"save\" in function_name.lower() or \"update\" in function_name.lower():\n",
    "        business_rules.append(\"Data persistence operations must consider transaction consistency.\")\n",
    "    if \"auth\" in function_name.lower() or \"login\" in function_name.lower():\n",
    "        business_rules.append(\"User authentication and authorization operations must follow security best practices.\")\n",
    "\n",
    "    qa_data = {\n",
    "        \"id\": qa_id,\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"code_context\": [\n",
    "            {\n",
    "                \"file_path\": os.path.relpath(file_path, CODE_REPO_PATH),\n",
    "                \"line_start\": start_line,\n",
    "                \"line_end\": end_line,\n",
    "                \"snippet\": code_snippet.strip()\n",
    "            }\n",
    "        ],\n",
    "        \"business_rules_context\": business_rules,\n",
    "        \"inference_trace\": inference_trace,\n",
    "        \"metadata\": {\n",
    "            \"source_module\": os.path.dirname(os.path.relpath(file_path, CODE_REPO_PATH)),\n",
    "            \"language\": \"python\",\n",
    "            \"difficulty\": \"llm_generated\", # Mark as LLM generated\n",
    "            \"timestamp\": \"2025-12-09T\" + os.popen('date -u +\"%H:%M:%SZ\"').read().strip(),\n",
    "            \"version_control_hash\": \"dummy_hash_for_example\"\n",
    "        }\n",
    "    }\n",
    "    return qa_data\n",
    "\n",
    "# --- Main Script ---\n",
    "def main():\n",
    "    if not os.path.exists(CODE_REPO_PATH):\n",
    "        print(f\"Error: Code repository path `{CODE_REPO_PATH}` does not exist. Please modify CODE_REPO_PATH to your local Python repository path.\")\n",
    "        print(\"Attempting to create a simple dummy repository for demonstration...\")\n",
    "        os.makedirs(CODE_REPO_PATH, exist_ok=True)\n",
    "        with open(os.path.join(CODE_REPO_PATH, \"my_module.py\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\"\"\n",
    "def calculate_sum(a: int, b: int) -> int:\n",
    "    \\\"\\\"\\\"\n",
    "    Calculates the sum of two integers.\n",
    "    This function takes two integers as input and returns their sum.\n",
    "    \\\"\\\"\\\"\n",
    "    return a + b\n",
    "\n",
    "def process_data(data_list: list):\n",
    "    # This function processes a list of data without a docstring.\n",
    "    print(f\"Processing {len(data_list)} items.\")\n",
    "    for item in data_list:\n",
    "        if item % 2 == 0:\n",
    "            print(f\"Even item: {item}\")\n",
    "        else:\n",
    "            print(f\"Odd item: {item}\")\n",
    "\n",
    "class MyManager:\n",
    "    \\\"\\\"\\\"\n",
    "    A class for managing resources.\n",
    "    Provides Create, Read, Update, and Delete (CRUD) operations for resources.\n",
    "    \\\"\\\"\\\"\n",
    "    def __init__(self, resource_name: str):\n",
    "        self.resource_name = resource_name\n",
    "        self.resources = []\n",
    "\n",
    "    def create_resource(self, resource_data: dict):\n",
    "        \\\"\\\"\\\"\n",
    "        Creates a new resource and adds it to the manager.\n",
    "        :param resource_data: Dictionary data of the resource.\n",
    "        :return: None\n",
    "        \\\"\\\"\\\"\n",
    "        self.resources.append(resource_data)\n",
    "        print(f\"Resource created: {resource_data}\")\n",
    "\n",
    "    def get_resource(self, resource_id: str) -> Optional[dict]:\n",
    "        # Retrieves a resource with the specified ID from the manager.\n",
    "        # This is an example method for finding resources.\n",
    "        for res in self.resources:\n",
    "            if res.get(\"id\") == resource_id:\n",
    "                return res\n",
    "        return None\n",
    "\"\"\")\n",
    "        print(f\"Dummy repository created at `{CODE_REPO_PATH}`.\")\n",
    "\n",
    "    all_qa_data = []\n",
    "    qa_id_counter = [0]\n",
    "\n",
    "    for root, _, files in os.walk(CODE_REPO_PATH):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "                functions_info = extract_function_info(file_path)\n",
    "                for func_info in functions_info:\n",
    "                    qa_entry = generate_qa_for_function(file_path, func_info, qa_id_counter)\n",
    "                    if qa_entry:\n",
    "                        all_qa_data.append(qa_entry)\n",
    "\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in all_qa_data:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"\\nTraining data generated and saved to `{OUTPUT_FILE}`. Total {len(all_qa_data)} records generated.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen-plus LLM client initialized successfully.\n",
      "\n",
      "Generating design solution for requirement: Add an asynchronous inventory deduction service to the existing order system to improve order processing response speed and ensure eventual consistency of inventory data.\n",
      "\n",
      "Generating design solution for requirement: Implement a user permission management module that supports role-permission assignment and provides APIs for permission verification.\n",
      "\n",
      "Generating design solution for requirement: Optimize the payment process, introduce a retry mechanism and idempotency handling to improve payment success rate.\n",
      "\n",
      "Generating design solution for requirement: Add a unified error logging and monitoring alert mechanism to the system.\n",
      "\n",
      "Generating design solution for requirement: Decouple user registration and login functionality from the existing user management module to form an independent authentication service.\n",
      "\n",
      "Training data generated and saved to `scenario2_design_data_with_llm_en.jsonl`. Total 5 records generated.\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "from openai import OpenAI\n",
    "import getpass\n",
    "import datetime\n",
    "\n",
    "# --- Configuration ---\n",
    "CODE_REPO_PATH = \"./my_python_repo\" # Replace with your local Python code repository path\n",
    "OUTPUT_FILE = \"scenario2_design_data_with_llm_en.jsonl\"\n",
    "LLM_MODEL_NAME = \"qwen-plus\" # Use your specified model\n",
    "REQUIREMENTS = [\n",
    "    {\"req\": \"Add an asynchronous inventory deduction service to the existing order system to improve order processing response speed and ensure eventual consistency of inventory data.\", \"keywords\": [\"order\", \"inventory\", \"async\"]},\n",
    "    {\"req\": \"Implement a user permission management module that supports role-permission assignment and provides APIs for permission verification.\", \"keywords\": [\"user\", \"auth\", \"permission\", \"role\"]},\n",
    "    {\"req\": \"Optimize the payment process, introduce a retry mechanism and idempotency handling to improve payment success rate.\", \"keywords\": [\"payment\", \"retry\", \"idempotent\"]},\n",
    "    {\"req\": \"Add a unified error logging and monitoring alert mechanism to the system.\", \"keywords\": [\"log\", \"monitor\", \"error\"]},\n",
    "    {\"req\": \"Decouple user registration and login functionality from the existing user management module to form an independent authentication service.\", \"keywords\": [\"user\", \"register\", \"login\", \"auth\", \"service\"]},\n",
    "]\n",
    "\n",
    "# --- OpenAI Client Initialization ---\n",
    "DASHSCOPE_API_KEY = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "if not DASHSCOPE_API_KEY:\n",
    "    DASHSCOPE_API_KEY = getpass.getpass(\"Please enter your DASHSCOPE_API_KEY: \")\n",
    "\n",
    "try:\n",
    "    llm_client = OpenAI(\n",
    "        api_key=DASHSCOPE_API_KEY,\n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\", # Beijing region base_url\n",
    "    )\n",
    "    print(\"Qwen-plus LLM client initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: LLM client initialization failed - {e}\")\n",
    "    print(\"Please refer to the documentation: https://help.aliyun.com/en/model-studio/developer-reference/error-code\")\n",
    "    llm_client = None\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def get_repo_file_list(repo_path: str, keywords: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieves relative paths of all Python files in the code repository and\n",
    "    simulates filtering relevant files based on keywords.\n",
    "    In a real-world scenario, this would involve complex knowledge graph queries or vector searches.\n",
    "    \"\"\"\n",
    "    relevant_files = []\n",
    "    all_files = []\n",
    "    for root, _, files in os.walk(repo_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(\".py\"):\n",
    "                relative_path = os.path.relpath(os.path.join(root, file_name), repo_path)\n",
    "                all_files.append(relative_path)\n",
    "\n",
    "                # Simulate keyword matching: consider a file relevant if its path or content contains keywords\n",
    "                # Note: For simplicity, only file paths and filenames are checked here.\n",
    "                is_relevant = False\n",
    "                for keyword in keywords:\n",
    "                    if keyword.lower() in relative_path.lower():\n",
    "                        is_relevant = True\n",
    "                        break\n",
    "                if is_relevant:\n",
    "                    relevant_files.append(relative_path)\n",
    "\n",
    "    # If no relevant files are found, return a small portion of all files as general context\n",
    "    if not relevant_files and all_files:\n",
    "        return all_files[:min(5, len(all_files))] # Returns up to the first 5 files\n",
    "    elif relevant_files:\n",
    "        return relevant_files\n",
    "    return []\n",
    "\n",
    "def read_file_content(repo_path: str, relative_file_path: str) -> str:\n",
    "    \"\"\"Reads the content of the specified file.\"\"\"\n",
    "    full_path = os.path.join(repo_path, relative_file_path)\n",
    "    try:\n",
    "        with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        return f\"# Error reading {relative_file_path}: {e}\"\n",
    "\n",
    "# --- LLM Integration Function for Scenario 2 ---\n",
    "def call_llm_for_design_solution(\n",
    "    requirement: str,\n",
    "    codebase_context: List[str], # List of relevant files\n",
    "    code_contents: Dict[str, str] # Contents of relevant files\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Calls the LLM to generate architectural design solutions, explanations, and inference traces.\n",
    "    \"\"\"\n",
    "    if not llm_client:\n",
    "        return {\n",
    "            \"design_solution\": \"LLM client not initialized, cannot generate design solution.\",\n",
    "            \"explanation\": \"LLM client initialization failed.\",\n",
    "            \"inference_trace\": \"LLM client initialization failed.\"\n",
    "        }\n",
    "\n",
    "    # Construct Prompt\n",
    "    system_prompt = (\n",
    "        \"You are a senior software architect and code expert, capable of providing detailed, reasonable, and scalable architectural design solutions based on given requirements and existing codebase information.\\n\"\n",
    "        \"Please first think step-by-step, analyze the requirements and existing context, then provide the design solution, explanation, and inference process.\\n\"\n",
    "        \"Your answer should consist of three parts: 'design_solution' (design solution), 'explanation' (explanation of the design solution), and 'inference_trace' (steps you thought through to arrive at the solution).\"\n",
    "    )\n",
    "\n",
    "    context_str = \"\"\n",
    "    if codebase_context:\n",
    "        context_str += \"\\n--- Relevant Files in Existing Codebase ---\\n\"\n",
    "        for i, file_path in enumerate(codebase_context):\n",
    "            context_str += f\"File {i+1}: {file_path}\\n\"\n",
    "            # In a real scenario, this would not directly include all file content,\n",
    "            # but rather extract key information via a knowledge graph or summarize using RAG.\n",
    "            # For demonstration purposes, we assume some file content can be provided here.\n",
    "            if file_path in code_contents:\n",
    "                # Only show a portion to avoid overly long prompts\n",
    "                content = code_contents[file_path]\n",
    "                context_str += f\"```python\\n{content[:500]}...\\n```\\n\" # Truncated\n",
    "            context_str += \"---\\n\"\n",
    "    else:\n",
    "        context_str += \"\\n--- Existing Codebase Information ---\\n\"\n",
    "        context_str += \"No code files directly relevant to the requirement found. Please design based on general design principles and best practices.\\n\"\n",
    "\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"Please design an architectural solution based on the following requirements and provided existing codebase context. Please note:\\n\"\n",
    "        f\"1. Your design should consider the characteristics of the existing Python codebase and potential directions for expansion.\\n\"\n",
    "        f\"2. The solution should be structured, clear, and include necessary explanations and inference processes.\\n\\n\"\n",
    "        f\"--- Requirement ---\\n\"\n",
    "        f\"{requirement}\\n\"\n",
    "        f\"{context_str}\\n\\n\"\n",
    "        f\"--- Output Format ---\\n\"\n",
    "        f\"Please return your answer in JSON format, including 'design_solution' (detailed architectural design solution, using Markdown format),\\n\"\n",
    "        f\" 'explanation' (explanation of the design solution) and 'inference_trace' (steps you thought through to arrive at the solution).\\n\"\n",
    "        f\"Example:\\n\"\n",
    "        f\"```json\\n\"\n",
    "        f\"{{\\n\"\n",
    "        f\"  \\\"design_solution\\\": \\\"# Solution Title\\\\n1. ...\\\\n2. ...\\\",\\n\"\n",
    "        f\"  \\\"explanation\\\": \\\"The advantages of this solution are...\\\",\\n\"\n",
    "        f\"  \\\"inference_trace\\\": \\\"1. First I analyzed the requirements...\\\\n2. Next I evaluated the existing system...\\\\n3. Finally I proposed...\\\"\\n\"\n",
    "        f\"}}\\n\"\n",
    "        f\"```\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        completion = llm_client.chat.completions.create(\n",
    "            model=LLM_MODEL_NAME,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': system_prompt},\n",
    "                {'role': 'user', 'content': user_prompt}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        llm_response_content = completion.choices[0].message.content\n",
    "\n",
    "        try:\n",
    "            parsed_response = json.loads(llm_response_content)\n",
    "            return {\n",
    "                \"design_solution\": parsed_response.get(\"design_solution\", \"LLM did not provide a design solution.\"),\n",
    "                \"explanation\": parsed_response.get(\"explanation\", \"LLM did not provide an explanation.\"),\n",
    "                \"inference_trace\": parsed_response.get(\"inference_trace\", \"LLM did not provide inference trace.\")\n",
    "            }\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: LLM returned invalid JSON format.\\nRaw response: {llm_response_content}\")\n",
    "            return {\n",
    "                \"design_solution\": f\"LLM returned invalid format, raw response: {llm_response_content}\",\n",
    "                \"explanation\": \"LLM returned invalid JSON.\",\n",
    "                \"inference_trace\": \"LLM returned invalid JSON.\"\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"LLM call failed: {e}\")\n",
    "        return {\n",
    "            \"design_solution\": f\"LLM call failed, error: {e}\",\n",
    "            \"explanation\": f\"LLM call failed, error: {e}\",\n",
    "            \"inference_trace\": f\"LLM call failed, error: {e}\"\n",
    "        }\n",
    "\n",
    "def generate_design_data(\n",
    "    design_id_counter: List[int],\n",
    "    requirement: str,\n",
    "    repo_path: str,\n",
    "    # Simulates extracting keywords from requirements; real-world scenarios would be more complex\n",
    "    requirement_keywords: List[str]\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generates a data point for an architectural design solution.\n",
    "    \"\"\"\n",
    "    if not requirement.strip():\n",
    "        return None\n",
    "\n",
    "    design_id_counter[0] += 1\n",
    "    design_id = f\"design_py_{design_id_counter[0]:05d}\"\n",
    "\n",
    "    # Simulate context extraction: get a list of code files relevant to the requirement keywords\n",
    "    codebase_context_files = get_repo_file_list(repo_path, requirement_keywords)\n",
    "\n",
    "    # For LLM calls, the content of these files needs to be read (only a portion is truncated here to avoid overly long prompts)\n",
    "    code_contents_for_llm = {\n",
    "        f: read_file_content(repo_path, f) for f in codebase_context_files\n",
    "    }\n",
    "\n",
    "    # Call LLM to generate the design solution\n",
    "    llm_output = call_llm_for_design_solution(\n",
    "        requirement,\n",
    "        codebase_context_files,\n",
    "        code_contents_for_llm\n",
    "    )\n",
    "\n",
    "    design_data = {\n",
    "        \"id\": design_id,\n",
    "        \"requirement\": requirement,\n",
    "        \"design_solution\": llm_output[\"design_solution\"],\n",
    "        \"explanation\": llm_output[\"explanation\"],\n",
    "        \"inference_trace\": llm_output[\"inference_trace\"],\n",
    "        \"codebase_context\": codebase_context_files, # Records the list of relevant files seen by the LLM\n",
    "        \"metadata\": {\n",
    "            \"source_project\": os.path.basename(repo_path),\n",
    "            \"design_type\": \"feature_extension\", # Default type, can be determined by LLM or defined based on requirements\n",
    "            \"language\": \"python\",\n",
    "            \"difficulty\": \"llm_generated\",\n",
    "            \"timestamp\": datetime.datetime.now(datetime.timezone.utc).isoformat(),\n",
    "            \"version_control_hash\": \"dummy_hash_for_example\" # Should actually retrieve git commit hash\n",
    "        }\n",
    "    }\n",
    "    return design_data\n",
    "\n",
    "# --- Main Script ---\n",
    "def main():\n",
    "    if not os.path.exists(CODE_REPO_PATH):\n",
    "        print(f\"Error: Code repository path `{CODE_REPO_PATH}` does not exist. Please modify CODE_REPO_PATH to your local Python repository path.\")\n",
    "        print(\"Attempting to create a simple dummy repository for demonstration...\")\n",
    "        os.makedirs(CODE_REPO_PATH, exist_ok=True)\n",
    "        with open(os.path.join(CODE_REPO_PATH, \"__init__.py\"), \"w\") as f: f.write(\"\")\n",
    "        with open(os.path.join(CODE_REPO_PATH, \"user_management.py\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\"\"\n",
    "def register_user(username, password, email):\n",
    "    # This registers a new user\n",
    "    print(f\"Registering {username}\")\n",
    "    # ... database logic\n",
    "    return {\"id\": 1, \"username\": username}\n",
    "\n",
    "def get_user_profile(user_id):\n",
    "    # Retrieves user profile from DB\n",
    "    return {\"id\": user_id, \"username\": \"test_user\"}\n",
    "\"\"\")\n",
    "        with open(os.path.join(CODE_REPO_PATH, \"order_service.py\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\"\"\n",
    "def create_order(user_id, items):\n",
    "    # Creates a new order\n",
    "    print(f\"Creating order for user {user_id}\")\n",
    "    # ... inventory check, payment processing\n",
    "    return {\"order_id\": \"ORD001\", \"status\": \"pending\"}\n",
    "\n",
    "def update_order_status(order_id, new_status):\n",
    "    # Updates an existing order's status\n",
    "    print(f\"Updating order {order_id} to {new_status}\")\n",
    "    return True\n",
    "\"\"\")\n",
    "        with open(os.path.join(CODE_REPO_PATH, \"payment_gateway.py\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\"\"\n",
    "def process_payment(order_id, amount, payment_method):\n",
    "    # Integrates with external payment provider\n",
    "    print(f\"Processing payment for order {order_id}, amount {amount}\")\n",
    "    return {\"success\": True, \"transaction_id\": \"TXN123\"}\n",
    "\"\"\")\n",
    "        print(f\"Dummy repository created at `{CODE_REPO_PATH}`.\")\n",
    "\n",
    "\n",
    "\n",
    "    all_design_data = []\n",
    "    design_id_counter = [0]\n",
    "\n",
    "    for req_item in REQUIREMENTS:\n",
    "        print(f\"\\nGenerating design solution for requirement: {req_item['req']}\")\n",
    "        design_entry = generate_design_data(\n",
    "            design_id_counter,\n",
    "            req_item[\"req\"],\n",
    "            CODE_REPO_PATH,\n",
    "            req_item[\"keywords\"]\n",
    "        )\n",
    "        if design_entry:\n",
    "            all_design_data.append(design_entry)\n",
    "\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in all_design_data:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"\\nTraining data generated and saved to `{OUTPUT_FILE}`. Total {len(all_design_data)} records generated.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
